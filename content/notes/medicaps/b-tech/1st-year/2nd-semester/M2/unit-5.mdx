---
title: "Unit 5: Engineering Mathematics 2"
description: Errors and Approximations, Solution of Algebraic and Transcendental Equations (Regula Falsi, Newton-Raphson and Iterative methods), Solution of Simultaneous linear equations by Gauss Elimination, Gauss Jordan, Jacobiâ€™s and Gauss-Siedel Iterative methods.
date: 2025-01-12
tags: ["Engineering Mathematics 2", "2nd Semester", "1st Year", "B Tech"]
published: true
metadata:
  university: "Medicaps University"
  degree: "B Tech"
  semester: "2nd Semester"
  subject: "Engineering Mathematics 2"
---

---
## Errors and Approximations

In numerical methods, **errors** and **approximations** play a crucial role in determining the accuracy and reliability of computed results. This topic is essential when dealing with real-world problems, where exact solutions are often not feasible, and approximate methods are employed instead.

### Types of Errors

1. **Absolute Error**
   The absolute error is the difference between the exact value $$ x $$ and the approximate value $$ \hat{x} $$.

   $$ \text{Absolute Error} = |x - \hat{x}| $$

   - Where $$ x $$ is the exact value and $$ \hat{x} $$ is the approximate value.

2. **Relative Error**
   The relative error is the absolute error divided by the exact value. It gives a sense of how significant the error is relative to the true value.

   $$ \text{Relative Error} = \frac{|x - \hat{x}|}{|x|} $$

   - It is often expressed as a percentage:
   
   $$ \text{Relative Error (\%)} = \frac{|x - \hat{x}|}{|x|} \times 100 $$

3. **Percentage Error**
   The percentage error is the relative error multiplied by 100. It gives the error as a percentage of the true value.

   $$ \text{Percentage Error (\%)} = \frac{|x - \hat{x}|}{|x|} \times 100 $$

4. **Round-off Error**
   Round-off error arises when numbers are rounded to a finite number of decimal places. This happens due to the limited precision of computer representations.

   - For example, rounding $$ 3.14159 $$ to 2 decimal places gives $$ 3.14 $$, which introduces an error.

5. **Truncation Error**
   Truncation error occurs when an infinite process is approximated by a finite process. This happens when, for instance, an infinite series is truncated at a certain point.

   - Example: When approximating $$ e^x $$ using the Taylor series, truncating the series after a few terms introduces truncation error.

### Approximations

1. **Taylor Series Approximation**
   The **Taylor series** expansion is a powerful tool for approximating functions around a point $$ a $$. The general form of the Taylor series for a function $$ f(x) $$ around $$ x = a $$ is:

   $$ f(x) = f(a) + f'(a)(x - a) + \frac{f''(a)}{2!}(x - a)^2 + \cdots $$

   The approximation of $$ f(x) $$ after $$ n $$ terms is:

   $$ f(x) \approx f(a) + f'(a)(x - a) + \frac{f''(a)}{2!}(x - a)^2 + \cdots + \frac{f^{(n)}(a)}{n!}(x - a)^n $$

   - The more terms we include, the more accurate the approximation will be, but it may also increase computational complexity.

2. **Binomial Approximation**
   The **binomial approximation** is useful when approximating expressions of the form $$ (1 + x)^n $$, where $$ |x| < 1 $$. The binomial series expansion is:

   $$ (1 + x)^n \approx 1 + nx $$

   This approximation is useful for small $$ x $$ when $$ n $$ is a real number.

3. **Numerical Integration Approximations**
   Numerical methods often require approximating integrals. Some common methods for approximating definite integrals are:

   - **Trapezoidal Rule:**

     $$ \int_a^b f(x) \, dx \approx \frac{h}{2} \left( f(a) + f(b) \right) $$

     where $$ h = b - a $$ is the width of the interval.

   - **Simpson's Rule:**

     $$ \int_a^b f(x) \, dx \approx \frac{h}{3} \left( f(a) + 4f\left(\frac{a + b}{2}\right) + f(b) \right) $$

     where $$ h = b - a $$ is the width of the interval.

4. **Euler's Method for Solving ODEs**
   Euler's method is an approximation technique for solving ordinary differential equations (ODEs). The method approximates the solution of the differential equation:

   $$ y'(x) = f(x, y) $$

   using the formula:

   $$ y_{n+1} = y_n + h f(x_n, y_n) $$

   where $$ h $$ is the step size, and $$ y_n $$ is the approximation of the solution at $$ x_n $$.

---

### ðŸ’¡ **TIP:**  
- **Higher-order approximations** (more terms in a series) generally improve accuracy but also increase computational effort.
- **Error analysis** is crucial in numerical methods to determine how much the approximations differ from the exact solution and how to minimize errors.

---

### Summary of Key Concepts

| Concept                 | Description                                         |
|-------------------------|-----------------------------------------------------|
| **Absolute Error**       | The difference between the exact and approximate values. |
| **Relative Error**       | The absolute error relative to the exact value. |
| **Percentage Error**     | The relative error expressed as a percentage. |
| **Round-off Error**      | Error due to the finite precision of numbers. |
| **Truncation Error**     | Error due to approximating an infinite process by a finite one. |
| **Taylor Series Approximation** | Expands a function as an infinite series around a point. |
| **Binomial Approximation** | Approximates expressions of the form $$ (1 + x)^n $$. |
| **Numerical Integration** | Approximates integrals using methods like Trapezoidal and Simpsonâ€™s Rule. |
| **Eulerâ€™s Method**       | A method for numerically solving ordinary differential equations. |

---
## Solution of Algebraic and Transcendental Equations

Algebraic and transcendental equations are commonly encountered in numerical analysis, where exact solutions might not always be feasible. To obtain approximate solutions, we use various **root-finding methods**. Three important methods for solving these equations are the **Regula Falsi Method**, **Newton-Raphson Method**, and **Iterative Methods**.

### 1. **Regula Falsi Method (False Position Method)**

The **Regula Falsi method** is a bracketing method used to find the root of an equation. It is based on the intermediate value theorem and the concept of linear interpolation. The method improves the approximation of the root by iteratively refining the interval containing the root.

#### Steps for the Regula Falsi Method:
1. **Start with two initial approximations**, $$ x_0 $$ and $$ x_1 $$, such that $$ f(x_0) $$ and $$ f(x_1) $$ have opposite signs (i.e., $$ f(x_0) \cdot f(x_1) < 0 $$).
2. **Compute the new approximation** using the formula:

   $$
   x_2 = x_1 - \frac{f(x_1)(x_1 - x_0)}{f(x_1) - f(x_0)}
   $$

3. **Check the sign** of $$ f(x_2) $$:
   - If $$ f(x_0) \cdot f(x_2) < 0 $$, then the root lies between $$ x_0 $$ and $$ x_2 $$, so set $$ x_1 = x_2 $$.
   - If $$ f(x_1) \cdot f(x_2) < 0 $$, then the root lies between $$ x_1 $$ and $$ x_2 $$, so set $$ x_0 = x_2 $$.
   
4. **Repeat the process** until the desired level of accuracy is reached.

#### Example:

For solving $$ f(x) = x^2 - 4 = 0 $$, starting with $$ x_0 = 1 $$ and $$ x_1 = 3 $$:

1. Compute $$ x_2 $$ using the formula.
2. Check the sign of $$ f(x_2) $$ and update the interval.
3. Repeat until the root converges.

### 2. **Newton-Raphson Method**

The **Newton-Raphson method** is an iterative method that uses both the function and its derivative to approximate the root of the equation. This method converges faster than other methods (like the bisection method) if the initial guess is close to the root and the function is sufficiently smooth.

#### Steps for the Newton-Raphson Method:
1. Start with an initial guess $$ x_0 $$.
2. Compute the next approximation using the formula:

   $$
   x_{n+1} = x_n - \frac{f(x_n)}{f'(x_n)}
   $$

3. Repeat the process until $$ |x_{n+1} - x_n| $$ is smaller than a desired tolerance.

#### Example:

For solving $$ f(x) = x^2 - 4 = 0 $$, we start with $$ x_0 = 3 $$:

1. Compute $$ f(x_0) = x_0^2 - 4 = 5 $$ and $$ f'(x_0) = 2x_0 = 6 $$.
2. Calculate the next approximation:

   $$
   x_1 = x_0 - \frac{f(x_0)}{f'(x_0)} = 3 - \frac{5}{6} = 2.1667
   $$

3. Repeat the process until the approximation is sufficiently accurate.

### 3. **Iterative Methods**

Iterative methods are used to find approximate solutions to equations by repeatedly applying a function to an initial guess. These methods are typically used for algebraic and transcendental equations that cannot be solved explicitly.

#### General Steps for Iterative Methods:
1. Start with an initial guess $$ x_0 $$.
2. Use an iterative function $$ g(x) $$ such that:

   $$
   x_{n+1} = g(x_n)
   $$

   - The function $$ g(x) $$ is derived from the original equation, and the root of $$ g(x) = 0 $$ corresponds to the root of the original equation.

3. Repeat the iteration process until the difference $$ |x_{n+1} - x_n| $$ is smaller than a predetermined tolerance.

#### Example of Iterative Method:

For solving $$ f(x) = x^2 - 4 = 0 $$, we can rewrite the equation as:

$$
x = \frac{4}{x}
$$

This gives the iterative formula:

$$
x_{n+1} = \frac{4}{x_n}
$$

Starting with $$ x_0 = 3 $$:

1. Compute $$ x_1 = \frac{4}{3} \approx 1.3333 $$.
2. Compute $$ x_2 = \frac{4}{x_1} \approx 3.0000 $$.
3. Continue the iterations until the solution converges.

---

### ðŸ’¡ **TIP:**
- **Regula Falsi Method** works well for continuous functions with opposite signs at the endpoints of the interval.
- **Newton-Raphson Method** is faster but requires an initial guess close to the root and the function's derivative.
- **Iterative Methods** provide a simple approach but may require careful choice of the iteration function to ensure convergence.

---

### Summary of Key Concepts

| Method                    | Description                                           |
|---------------------------|-------------------------------------------------------|
| **Regula Falsi Method**    | A bracketing method that iteratively refines the interval containing the root using linear interpolation. |
| **Newton-Raphson Method**  | An iterative method that uses the function and its derivative to find the root. Converges quickly for smooth functions. |
| **Iterative Methods**      | Methods that repeatedly apply a function to an initial guess to find the root. |

---
## Solution of Simultaneous Linear Equations by Gauss Elimination

The **Gauss Elimination Method** is one of the most widely used techniques to solve a system of **simultaneous linear equations**. It is a direct method that systematically eliminates variables to reduce a system of equations to a simpler form, eventually finding the solutions.

### Steps in Gauss Elimination Method

The Gauss elimination method consists of two main steps: **forward elimination** and **back substitution**.

#### 1. **Forward Elimination**
   The goal of forward elimination is to transform the system of linear equations into an upper triangular matrix form. In an upper triangular matrix, all the elements below the diagonal are zero.

   Given a system of equations:
   $$
   a_{11}x_1 + a_{12}x_2 + \cdots + a_{1n}x_n = b_1
   $$
   $$
   a_{21}x_1 + a_{22}x_2 + \cdots + a_{2n}x_n = b_2
   $$
   $$ 
   \vdots
   $$
   $$
   a_{n1}x_1 + a_{n2}x_2 + \cdots + a_{nn}x_n = b_n
   $$

   The matrix form of the system is:

   $$
   A \cdot X = B
   $$

   Where:
   - $$ A $$ is the coefficient matrix.
   - $$ X $$ is the vector of unknowns $$ x_1, x_2, \dots, x_n $$.
   - $$ B $$ is the constant vector on the right-hand side.

   To apply Gauss elimination, we perform **row operations** to transform the system into an upper triangular matrix form:

   1. **Make the first element of the first column (i.e., $$ a_{11} $$) equal to 1** by dividing the entire first row by $$ a_{11} $$ (if $$ a_{11} \neq 0 $$).
   
   2. **Eliminate the first variable $$ x_1 $$ from the remaining rows** by using the first row to update the other rows. For each subsequent row $$ i $$, subtract a suitable multiple of the first row from row $$ i $$, such that the first element of each row becomes zero.

   3. Repeat the process for each subsequent column: For column $$ j $$, make the $$ a_{jj} $$ element (the pivot element) equal to 1 and use it to eliminate the variable $$ x_j $$ from the rows below.

#### 2. **Back Substitution**
   After the forward elimination step, the system will be in **upper triangular form**. In this form, it is easier to solve for the variables starting from the last row and working upwards.

   Suppose after the forward elimination, the system is reduced to:

   $$
   x_n = \frac{b_n}{a_{nn}}
   $$

   Then substitute $$ x_n $$ into the equation of the $$ n-1 $$ row:

   $$
   a_{(n-1)n}x_n + a_{(n-1)(n-1)}x_{n-1} = b_{n-1}
   $$

   Solve for $$ x_{n-1} $$, and repeat the process for all other variables until all variables $$ x_1, x_2, \dots, x_n $$ are found.

---

### Example: Solving a System of Linear Equations

Consider the following system of linear equations:

$$
2x + 3y - z = 1
$$
$$
4x + y + 2z = 2
$$
$$
3x + 2y + z = 3
$$

1. **Write the augmented matrix:**

   $$
   \begin{pmatrix}
   2 & 3 & -1 & | & 1 \\
   4 & 1 & 2 & | & 2 \\
   3 & 2 & 1 & | & 3
   \end{pmatrix}
   $$

2. **Perform forward elimination:**

   - First, we make the element at position $$ (1,1) $$ equal to 1 by dividing the first row by 2:

     $$
     \text{Row 1:} \quad \frac{1}{2} \times \text{Row 1} \Rightarrow \left(1, \frac{3}{2}, -\frac{1}{2}, |, \frac{1}{2}\right)
     $$

   - Now eliminate $$ x_1 $$ from the second and third rows:
     - Row 2: $$ \text{Row 2} - 4 \times \text{Row 1} \Rightarrow (0, -5, 4, |, 0) $$
     - Row 3: $$ \text{Row 3} - 3 \times \text{Row 1} \Rightarrow (0, -\frac{1}{2}, \frac{5}{2}, |, \frac{3}{2}) $$

   The updated augmented matrix is:

   $$
   \begin{pmatrix}
   1 & \frac{3}{2} & -\frac{1}{2} & | & \frac{1}{2} \\
   0 & -5 & 4 & | & 0 \\
   0 & -\frac{1}{2} & \frac{5}{2} & | & \frac{3}{2}
   \end{pmatrix}
   $$

3. **Eliminate $$ y $$ from the third row:**

   - Row 3: $$ \text{Row 3} - \left(-\frac{1}{10}\right) \times \text{Row 2} \Rightarrow (0, 0, 3, |, 1) $$

   The updated augmented matrix is:

   $$
   \begin{pmatrix}
   1 & \frac{3}{2} & -\frac{1}{2} & | & \frac{1}{2} \\
   0 & -5 & 4 & | & 0 \\
   0 & 0 & 3 & | & 1
   \end{pmatrix}
   $$

4. **Now, we perform back substitution:**

   From the third row, we have:

   $$
   3z = 1 \quad \Rightarrow \quad z = \frac{1}{3}
   $$

   Substitute $$ z = \frac{1}{3} $$ into the second row:

   $$
   -5y + 4z = 0 \quad \Rightarrow \quad -5y + \frac{4}{3} = 0 \quad \Rightarrow \quad y = \frac{4}{15}
   $$

   Finally, substitute $$ y = \frac{4}{15} $$ and $$ z = \frac{1}{3} $$ into the first row:

   $$
   x + \frac{3}{2}y - \frac{1}{2}z = \frac{1}{2}
   $$
   $$
   x + \frac{3}{2} \times \frac{4}{15} - \frac{1}{2} \times \frac{1}{3} = \frac{1}{2}
   $$
   $$
   x + \frac{6}{15} - \frac{1}{6} = \frac{1}{2} \quad \Rightarrow \quad x = \frac{7}{10}
   $$

Thus, the solution to the system is:

$$
x = \frac{7}{10}, \quad y = \frac{4}{15}, \quad z = \frac{1}{3}
$$

---

### ðŸ’¡ **TIP:**
- Gauss elimination is most efficient when the system of equations is well-conditioned (i.e., the coefficient matrix is not close to singular).
- The method can be applied to systems of any size, but its computational complexity increases with the number of equations.

---

### Summary of Key Concepts

| Concept                     | Description                                              |
|-----------------------------|----------------------------------------------------------|
| **Gauss Elimination Method** | A direct method for solving systems of linear equations by transforming the system into an upper triangular form. |
| **Forward Elimination**      | The process of eliminating variables to reduce the system to an upper triangular form. |
| **Back Substitution**        | The process of solving for variables starting from the last equation and working upwards. |

---
## Gauss-Jordan Method for Solving Simultaneous Linear Equations

The **Gauss-Jordan Method** is an extension of the **Gauss Elimination Method**. It is used to find the solution of a system of **simultaneous linear equations** by transforming the augmented matrix into a **diagonal matrix** (or reduced row echelon form). The key difference between Gauss-Jordan and Gauss Elimination is that Gauss-Jordan continues the process until the matrix is in a simpler form, where the coefficients of all variables become 1, making back substitution unnecessary.

### Steps in Gauss-Jordan Method

The Gauss-Jordan method consists of the following steps:

#### 1. **Write the Augmented Matrix**
   For the system of equations:

   $$
   a_{11}x_1 + a_{12}x_2 + \dots + a_{1n}x_n = b_1
   $$
   $$
   a_{21}x_1 + a_{22}x_2 + \dots + a_{2n}x_n = b_2
   $$
   $$
   \vdots
   $$
   $$
   a_{n1}x_1 + a_{n2}x_2 + \dots + a_{nn}x_n = b_n
   $$

   We write the augmented matrix as:

   $$
   \begin{pmatrix}
   a_{11} & a_{12} & \dots & a_{1n} & | & b_1 \\
   a_{21} & a_{22} & \dots & a_{2n} & | & b_2 \\
   \vdots & \vdots & \ddots & \vdots & \vdots & \vdots \\
   a_{n1} & a_{n2} & \dots & a_{nn} & | & b_n
   \end{pmatrix}
   $$

#### 2. **Perform Row Operations to Achieve Row Echelon Form**
   - **Step 1**: Begin with the first column (column 1). Make the diagonal element $$ a_{11} $$ equal to 1 by dividing the entire first row by $$ a_{11} $$ (if $$ a_{11} \neq 0 $$).
   - **Step 2**: Use the first row to eliminate all elements below the pivot (diagonal element) by making them zero. This can be done by appropriate row operations.
     - For example, for each row $$ i $$ (where $$ i > 1 $$):
       $$ \text{Row}_i \rightarrow \text{Row}_i - \left( \frac{a_{i1}}{a_{11}} \right) \times \text{Row}_1 $$

   Repeat the process for all columns, ensuring that each pivot element is 1 and all elements below and above the pivots are zero.

#### 3. **Continue Until the Matrix is in Reduced Row Echelon Form**
   - For each column $$ j $$, make the diagonal element $$ a_{jj} $$ equal to 1.
   - Use the rows to eliminate the elements above each pivot to make the matrix into a diagonal matrix.
   - Repeat the process for every column, until the matrix is in the following form:

   $$
   \begin{pmatrix}
   1 & 0 & 0 & \dots & 0 & | & x_1 \\
   0 & 1 & 0 & \dots & 0 & | & x_2 \\
   \vdots & \vdots & \ddots & \vdots & \vdots & \vdots \\
   0 & 0 & 0 & \dots & 1 & | & x_n
   \end{pmatrix}
   $$

   Here, the right-hand side of the augmented matrix will contain the solutions $$ x_1, x_2, \dots, x_n $$.

---

### Example: Solving a System of Linear Equations Using Gauss-Jordan

Consider the following system of linear equations:

$$
x + y + z = 6
$$
$$
2x + 3y + z = 14
$$
$$
3x + y + 2z = 13
$$

#### Step 1: Write the augmented matrix

$$
\begin{pmatrix}
1 & 1 & 1 & | & 6 \\
2 & 3 & 1 & | & 14 \\
3 & 1 & 2 & | & 13
\end{pmatrix}
$$

#### Step 2: Perform Row Operations to Get the First Pivot (1 at $$ a_{11} $$)

- The first row already has a 1 at the $$ a_{11} $$ position, so no changes are needed here.
- Now, eliminate the values below $$ a_{11} $$ (i.e., make the elements $$ a_{21} $$ and $$ a_{31} $$ zero):
  - Row 2: $$ \text{Row}_2 \rightarrow \text{Row}_2 - 2 \times \text{Row}_1 $$
  - Row 3: $$ \text{Row}_3 \rightarrow \text{Row}_3 - 3 \times \text{Row}_1 $$

The updated augmented matrix becomes:

$$
\begin{pmatrix}
1 & 1 & 1 & | & 6 \\
0 & 1 & -1 & | & 2 \\
0 & -2 & -1 & | & 5
\end{pmatrix}
$$

#### Step 3: Perform Row Operations for the Second Pivot

- The second pivot is already 1 at $$ a_{22} $$. Now eliminate the values below and above $$ a_{22} $$:
  - Row 3: $$ \text{Row}_3 \rightarrow \text{Row}_3 + 2 \times \text{Row}_2 $$

The updated augmented matrix becomes:

$$
\begin{pmatrix}
1 & 1 & 1 & | & 6 \\
0 & 1 & -1 & | & 2 \\
0 & 0 & -3 & | & 9
\end{pmatrix}
$$

#### Step 4: Perform Row Operations for the Third Pivot

- The third pivot is $$ -3 $$ at $$ a_{33} $$, so we divide Row 3 by $$ -3 $$ to make it 1:
  - Row 3: $$ \text{Row}_3 \rightarrow \frac{1}{-3} \times \text{Row}_3 $$

The updated augmented matrix becomes:

$$
\begin{pmatrix}
1 & 1 & 1 & | & 6 \\
0 & 1 & -1 & | & 2 \\
0 & 0 & 1 & | & -3
\end{pmatrix}
$$

#### Step 5: Back Substitution (Eliminate Elements Above Each Pivot)

- Now, eliminate the elements above each pivot:
  - Row 2: $$ \text{Row}_2 \rightarrow \text{Row}_2 + \text{Row}_3 $$
  - Row 1: $$ \text{Row}_1 \rightarrow \text{Row}_1 - \text{Row}_3 $$

The updated augmented matrix becomes:

$$
\begin{pmatrix}
1 & 1 & 0 & | & 9 \\
0 & 1 & 0 & | & -1 \\
0 & 0 & 1 & | & -3
\end{pmatrix}
$$

#### Step 6: Read the Solution

From the final augmented matrix, we can now read the solutions directly:

$$
x = 9, \quad y = -1, \quad z = -3
$$

Thus, the solution to the system of equations is:

$$
x = 9, \quad y = -1, \quad z = -3
$$

---

### ðŸ’¡ **TIP:**
- The Gauss-Jordan method is particularly useful when solving systems of equations where you need the inverse of a matrix, as it transforms the augmented matrix into the **identity matrix** and the right-hand side of the augmented matrix becomes the solution.
- **Row Operations** can sometimes result in a situation where the system is inconsistent or has infinitely many solutions, which can be identified by checking if a row has all zeros in the coefficient positions but a non-zero constant in the augmented column.

---

### Summary of Key Concepts

| Concept                    | Description                                                   |
|----------------------------|---------------------------------------------------------------|
| **Gauss-Jordan Method**     | A method for solving systems of linear equations by reducing the augmented matrix to a reduced row echelon form (diagonal form). |
| **Row Operations**          | The key operations used to reduce the augmented matrix: row swaps, scaling rows, and adding/subtracting rows. |
| **Reduced Row Echelon Form**| The final form of the matrix, where all diagonal elements are 1, and all elements above and below the pivots are 0. |

---
## Jacobiâ€™s and Gauss-Seidel Iterative Methods

Both **Jacobiâ€™s** and **Gauss-Seidel** methods are iterative techniques used to solve a system of **linear equations**. These methods are generally used when a system is too large to be solved by direct methods like **Gauss Elimination** or **Gauss-Jordan** due to computational complexity. These iterative methods provide an approximate solution to the system after a set number of iterations.

### 1. **Jacobiâ€™s Iterative Method**

Jacobi's method is an iterative technique to solve the system of linear equations in the form:

$$
a_{11}x_1 + a_{12}x_2 + \dots + a_{1n}x_n = b_1
$$
$$
a_{21}x_1 + a_{22}x_2 + \dots + a_{2n}x_n = b_2
$$
$$
\vdots
$$
$$
a_{n1}x_1 + a_{n2}x_2 + \dots + a_{nn}x_n = b_n
$$

The basic idea behind the Jacobi method is to start with an initial guess for the variables and iteratively improve the guess based on the previous values of the other variables. The equations are solved for each variable in isolation, and the values from the previous iteration are used in the calculation.

#### Steps for Jacobiâ€™s Method:

1. **Rearrange the equations** to isolate each variable on the left-hand side:
   $$
   x_1 = \frac{1}{a_{11}} \left( b_1 - (a_{12}x_2 + a_{13}x_3 + \dots + a_{1n}x_n) \right)
   $$
   $$
   x_2 = \frac{1}{a_{22}} \left( b_2 - (a_{21}x_1 + a_{23}x_3 + \dots + a_{2n}x_n) \right)
   $$
   $$
   \vdots
   $$
   $$
   x_n = \frac{1}{a_{nn}} \left( b_n - (a_{n1}x_1 + a_{n2}x_2 + \dots + a_{n(n-1)}x_{n-1}) \right)
   $$

2. **Set initial guesses** for $$ x_1^{(0)}, x_2^{(0)}, \dots, x_n^{(0)} $$.

3. **Iterate** using the updated formulas for $$ x_1^{(k)}, x_2^{(k)}, \dots, x_n^{(k)} $$, where $$ k $$ denotes the iteration number. Continue updating the variables until the values converge or until a pre-specified number of iterations is reached.

4. **Check for convergence**. The method converges if the system is diagonally dominant or the absolute difference between successive iterations is below a small threshold.

#### Example:

Consider the following system of equations:

$$
4x_1 - x_2 + x_3 = 3
$$
$$
2x_1 + 3x_2 + x_3 = 12
$$
$$
x_1 + 2x_2 + 3x_3 = 10
$$

1. Rearrange each equation to isolate the variables:
   $$
   x_1 = \frac{1}{4} \left( 3 + x_2 - x_3 \right)
   $$
   $$
   x_2 = \frac{1}{3} \left( 12 - 2x_1 - x_3 \right)
   $$
   $$
   x_3 = \frac{1}{3} \left( 10 - x_1 - 2x_2 \right)
   $$

2. Start with initial guesses: $$ x_1^{(0)} = 0, x_2^{(0)} = 0, x_3^{(0)} = 0 $$.

3. After the first iteration:

   - $$ x_1^{(1)} = \frac{1}{4} \left( 3 + 0 - 0 \right) = 0.75 $$
   - $$ x_2^{(1)} = \frac{1}{3} \left( 12 - 2(0) - 0 \right) = 4 $$
   - $$ x_3^{(1)} = \frac{1}{3} \left( 10 - 0 - 2(0) \right) = 3.33 $$

   Continue iterating until the values converge.

---

### 2. **Gauss-Seidel Iterative Method**

The Gauss-Seidel method is an improvement over the Jacobi method. In the Gauss-Seidel method, each variable is updated immediately after it is computed during the same iteration, which can lead to faster convergence.

#### Steps for Gauss-Seidel Method:

1. **Rearrange the equations** in the same way as for Jacobiâ€™s method to isolate each variable:
   $$
   x_1 = \frac{1}{a_{11}} \left( b_1 - (a_{12}x_2 + a_{13}x_3 + \dots + a_{1n}x_n) \right)
   $$
   $$
   x_2 = \frac{1}{a_{22}} \left( b_2 - (a_{21}x_1 + a_{23}x_3 + \dots + a_{2n}x_n) \right)
   $$
   $$
   \vdots
   $$
   $$
   x_n = \frac{1}{a_{nn}} \left( b_n - (a_{n1}x_1 + a_{n2}x_2 + \dots + a_{n(n-1)}x_{n-1}) \right)
   $$

2. **Set initial guesses** for $$ x_1^{(0)}, x_2^{(0)}, \dots, x_n^{(0)} $$.

3. **Iterate** using the updated values of $$ x_1^{(k)}, x_2^{(k)}, \dots, x_n^{(k)} $$, where $$ k $$ is the iteration number. Each new value of $$ x_i $$ is immediately used in the next calculation during the same iteration.

4. **Check for convergence**. The Gauss-Seidel method converges faster than Jacobiâ€™s method for most systems.

#### Example:

Using the same system of equations from the Jacobi example:

$$
4x_1 - x_2 + x_3 = 3
$$
$$
2x_1 + 3x_2 + x_3 = 12
$$
$$
x_1 + 2x_2 + 3x_3 = 10
$$

1. Rearrange each equation to isolate the variables:

   $$
   x_1 = \frac{1}{4} \left( 3 + x_2 - x_3 \right)
   $$
   $$
   x_2 = \frac{1}{3} \left( 12 - 2x_1 - x_3 \right)
   $$
   $$
   x_3 = \frac{1}{3} \left( 10 - x_1 - 2x_2 \right)
   $$

2. Start with initial guesses: $$ x_1^{(0)} = 0, x_2^{(0)} = 0, x_3^{(0)} = 0 $$.

3. After the first iteration:
   
   - $$ x_1^{(1)} = \frac{1}{4} \left( 3 + 0 - 0 \right) = 0.75 $$
   - $$ x_2^{(1)} = \frac{1}{3} \left( 12 - 2(0.75) - 0 \right) = 3.5 $$
   - $$ x_3^{(1)} = \frac{1}{3} \left( 10 - 0.75 - 2(3.5) \right) = 1.25 $$

   Continue iterating until the values converge.

---

### **Comparing Jacobiâ€™s and Gauss-Seidel Methods**

| **Aspect**                    | **Jacobi Method**                         | **Gauss-Seidel Method**                       |
|-------------------------------|-------------------------------------------|----------------------------------------------|
| **Iteration Process**          | Uses values from the previous iteration for all variables. | Uses updated values within the same iteration. |
| **Convergence Rate**           | Slower convergence.                      | Faster convergence (often preferred).         |
| **Memory Usage**               | Requires storing values from the previous iteration. | Does not require storing all previous iteration values. |
| **Computational Complexity**   | Typically slower and may require more iterations to converge. | Faster and requires fewer iterations.         |
| **Application**                | Useful for smaller systems or when simplicity is important. | More efficient for larger systems.            |

---

### ðŸ’¡ **TIP:**
- The **Jacobi method** can be used when parallel computation is required since each variable can be computed independently in each iteration.
- The **Gauss-Seidel method** is generally more efficient as it uses the most recent values during the iteration, thus leading to faster convergence.

---

### Summary of Key Concepts

| **Concept**                 | **Jacobi Method**                          | **Gauss-Seidel Method**                       |
|-----------------------------|--------------------------------------------|----------------------------------------------|
| **Iteration Scheme**         | Uses old values for all variables.         | Uses updated values immediately within the iteration. |
| **Convergence**              | Slower convergence.                       | Faster convergence.                          |
| **Memory Usage**             | Requires more memory to store previous iteration values. | Requires less memory as it uses updated values during the iteration. |

---