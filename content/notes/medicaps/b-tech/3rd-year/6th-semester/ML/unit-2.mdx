---
title: "Unit 2: Machine Learning"
description: Overfitting and Underfitting, Regularization, Bias and Variance, Decision Trees, Naive Bayes, Support Vector Machines, Kernel Methods.
date: 2025-01-19
tags: ["Machine Learning", "6th Semester", "3rd Year", "medicaps university"]
published: true
metadata:
  university: "Medicaps University"
  degree: "B Tech"
  semester: "6th Semester"
  subject: "Machine Learning"
---

---
## Overfitting and Underfitting

### What is Overfitting?
**Overfitting** occurs when a machine learning model learns the details and noise in the training data to such an extent that it negatively impacts the model‚Äôs performance on new, unseen data. Essentially, the model becomes too complex and fits the training data perfectly but fails to generalise to new data.

---

### Symptoms of Overfitting
- **High Accuracy on Training Data:** The model performs well on the training data, with very low error.
- **Poor Accuracy on Test Data:** The model performs poorly on the validation or test data, leading to high generalisation error.

---

### Causes of Overfitting
- **Model Complexity:** Using a model with too many parameters relative to the amount of training data. E.g., using a deep neural network for a small dataset.
- **Noise in Data:** The model may learn random fluctuations in the data as if they were meaningful patterns.
- **Insufficient Training Data:** The model may fit the training data too closely, as there is not enough data to detect general trends.

---

### Solutions to Overfitting
1. **Simplify the Model:**
   - Reduce the number of features (feature selection).
   - Use simpler models (e.g., linear models instead of complex neural networks).
   
2. **Use More Data:**
   - Increase the size of the training dataset to improve the model's ability to generalise.
   
3. **Regularisation:**
   - Add a penalty term to the loss function to constrain the model (e.g., L1 or L2 regularisation).
   
4. **Cross-Validation:**
   - Use techniques like k-fold cross-validation to ensure that the model is tested on different data splits.

5. **Pruning (in Decision Trees):**
   - Remove branches of the tree that add little predictive power.

---

### What is Underfitting?
**Underfitting** occurs when a machine learning model is too simple to capture the underlying patterns in the data. It happens when the model has too few parameters or is too restrictive, resulting in poor performance on both the training and test datasets.

---

### Symptoms of Underfitting
- **Low Accuracy on Training Data:** The model performs poorly on the training data, meaning it doesn't capture the underlying patterns.
- **Low Accuracy on Test Data:** The model continues to perform poorly on unseen data, suggesting it hasn‚Äôt learned the underlying relationships.

---

### Causes of Underfitting
- **Model Simplicity:** Using a model that is too simple, such as a linear model for a non-linear problem.
- **Insufficient Training:** Not training the model long enough or with enough data to learn from the patterns.
- **Excessive Regularisation:** Applying too much regularisation, which limits the model's capacity to fit the data.

---

### Solutions to Underfitting
1. **Use a More Complex Model:**
   - Increase model complexity (e.g., using polynomial regression or deep learning models).
   
2. **Feature Engineering:**
   - Add more relevant features or perform transformations that can help the model capture patterns.
   
3. **Remove Regularisation:**
   - Decrease the regularisation strength to allow the model to better fit the training data.

4. **Train for More Epochs:**
   - Allow the model to train longer to improve its performance.

---

### Bias-Variance Tradeoff
Overfitting and underfitting are both related to the **bias-variance tradeoff**:
- **High Bias (Underfitting):** The model is too simple and cannot capture the underlying patterns in the data.
- **High Variance (Overfitting):** The model is too complex and learns noise or fluctuations in the data, making it highly sensitive to changes in the dataset.

The goal is to find a balance between bias and variance to achieve good generalisation.

---

### Visualisation of Overfitting and Underfitting

**Overfitting:**
- The model fits the training data too well, capturing noise and fluctuations.
- **Graph:** A highly complex curve fitting each data point perfectly.

**Underfitting:**
- The model is too simple to capture the underlying data trends.
- **Graph:** A straight line that misses the data patterns.

---

### Example of Overfitting vs. Underfitting
#### Problem: Predicting House Prices
- **Overfitting:** Using a very high-degree polynomial regression model on a small dataset results in a curve that fits every data point but performs poorly on new data.
- **Underfitting:** Using linear regression when the data has non-linear relationships results in poor predictions on both the training and test data.

#### üìù **NOTE:** Striking the right balance between model complexity and data variability is key to achieving optimal performance.
---

## Regularisation

### What is Regularisation?
**Regularisation** is a technique used in machine learning to prevent overfitting by adding a penalty to the model's complexity. It helps ensure that the model generalises well to new, unseen data by discouraging it from fitting noise or overly complex patterns in the training data.

---

### Types of Regularisation
There are two common types of regularisation techniques used in machine learning:

1. **L1 Regularisation (Lasso):**
   - L1 regularisation adds a penalty proportional to the absolute value of the model parameters.
   - It encourages sparsity in the model by driving some of the model‚Äôs coefficients to zero, effectively selecting a subset of features.
   
   The L1 regularisation term is:

   L1 Penalty = $$\lambda \sum_{i=1}^{n} |\theta_i|$$

   Where:
   - $$ \theta_i $$: Model parameters (weights).
   - $$ \lambda $$: Regularisation parameter (controls the strength of the penalty).
   - $$ n $$: Number of features.

   **Advantages:**
   - Performs feature selection by shrinking some weights to zero.
   - Useful when dealing with high-dimensional data.

2. **L2 Regularisation (Ridge):**
   - L2 regularisation adds a penalty proportional to the square of the model parameters.
   - It discourages large values of the parameters but does not make them exactly zero. It shrinks the coefficients towards zero without setting them to exactly zero.
   
   The L2 regularisation term is:

   L2 Penalty = $$\lambda \sum_{i=1}^{n} \theta_i^2$$
   
   Where:
   - $$ \theta_i $$: Model parameters.
   - $$ \lambda $$: Regularisation parameter.
   - $$ n $$: Number of features.

   **Advantages:**
   - Prevents large coefficients and reduces overfitting.
   - Helps in cases where there are many features but no obvious sparsity.

---

### Elastic Net Regularisation
Elastic Net is a combination of both L1 and L2 regularisation. It blends the properties of Lasso and Ridge regularisation, providing a balance between feature selection and coefficient shrinkage.

The Elastic Net regularisation term is:

$$
\text{Elastic Net Penalty} = \lambda_1 \sum_{i=1}^{n} |\theta_i| + \lambda_2 \sum_{i=1}^{n} \theta_i^2
$$

Where:
- $$ \lambda_1 $$: L1 regularisation strength.
- $$ \lambda_2 $$: L2 regularisation strength.

**Advantages:**
- Useful when there are multiple correlated features.
- Performs well when there is both feature selection and regularisation needed.

---

### How Regularisation Works
- **Overfitting:** In the absence of regularisation, a model may fit the training data too perfectly, leading to overfitting.
- **Regularisation:** By adding a penalty term to the cost function, regularisation discourages large model weights, effectively controlling the model's complexity and making it more generalisable.

For example, in Linear Regression, the cost function with L2 regularisation becomes:

$$
J(\theta) = \frac{1}{2m} \sum_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)})^2 + \frac{\lambda}{2m} \sum_{j=1}^{n} \theta_j^2
$$

Where:
- The first term is the standard mean squared error (MSE) loss.
- The second term is the L2 penalty added to the cost function.

---

### Regularisation Parameter ($$ \lambda $$)
- $$ \lambda $$ controls the strength of the penalty applied to the model. A larger $$ \lambda $$ will result in a stronger penalty and more regularisation, leading to smaller model weights.
- If $$ \lambda = 0 $$, there is no regularisation, and the model behaves like standard linear regression.
- If $$ \lambda $$ is very large, the model will be overly constrained and may underfit.

---

### Regularisation in Machine Learning Models
1. **Linear Regression:**  
   - L1 regularisation (Lasso) and L2 regularisation (Ridge) are commonly used to improve generalisation in linear models.

2. **Logistic Regression:**  
   - Regularisation is applied to the logistic regression cost function to prevent overfitting, especially when dealing with a large number of features.

3. **Neural Networks:**  
   - Regularisation techniques like L2 regularisation, Dropout, and Early Stopping are used in deep learning models to prevent overfitting.

4. **Support Vector Machines (SVM):**  
   - Regularisation is used to control the margin width between the support vectors, balancing model complexity and generalisation.

---

### Example: Regularised Linear Regression
```python
from sklearn.linear_model import Ridge, Lasso

# Create model with L2 regularisation (Ridge)
model_ridge = Ridge(alpha=0.1)  # alpha corresponds to lambda in the formula
model_ridge.fit(X_train, y_train)

# Create model with L1 regularisation (Lasso)
model_lasso = Lasso(alpha=0.1)  # alpha corresponds to lambda in the formula
model_lasso.fit(X_train, y_train)
```
#### ‚ö†Ô∏è **CAUTION:** Regularisation requires tuning of the $$ \lambda $$ parameter, and cross-validation is often used to select the optimal value for better model performance.

#### üìù **NOTE:** Regularisation is a powerful technique to improve the performance of machine learning models by preventing overfitting and improving generalisation.
---

## Bias and Variance

### What is Bias?
**Bias** refers to the error introduced by approximating a real-world problem (which may be very complex) by a simplified model. A high-bias model makes strong assumptions about the data and tends to underfit, meaning it fails to capture the underlying patterns.

- **High Bias:** The model is too simplistic and cannot capture the complexity of the data.
- **Low Bias:** The model makes fewer assumptions and can capture more complex patterns.

#### Examples of High Bias:
- A linear model fitting data that has a non-linear relationship.
- Using overly simple models on complex problems (e.g., linear regression on a classification task).

#### Consequences of High Bias:
- **Underfitting:** The model is too simplistic and doesn't perform well on both training and test data.
- **Systematic Errors:** The model consistently makes incorrect predictions, leading to poor accuracy.

---

### What is Variance?
**Variance** refers to the model's sensitivity to the specific data points in the training set. A high-variance model is complex and can capture a lot of noise in the data, leading to overfitting.

- **High Variance:** The model is too sensitive to fluctuations in the training data and captures noise, leading to overfitting.
- **Low Variance:** The model generalises better and doesn't overfit the training data.

#### Examples of High Variance:
- A decision tree with many branches that fits the training data perfectly but doesn't generalise well to unseen data.
- A complex neural network with too many parameters and little regularisation.

#### Consequences of High Variance:
- **Overfitting:** The model performs exceptionally well on training data but poorly on test data.
- **Unstable Predictions:** The model‚Äôs predictions fluctuate significantly with small changes in the training data.

---

### Bias-Variance Tradeoff
The goal in machine learning is to find the right balance between **bias** and **variance**:

- **High Bias (Underfitting):** Occurs when the model is too simple and cannot capture the patterns in the data.
- **High Variance (Overfitting):** Occurs when the model is too complex and captures noise, leading to poor generalisation.

#### The Tradeoff:
- **Low Bias, High Variance:** The model fits the training data well but may overfit.
- **High Bias, Low Variance:** The model doesn't fit the training data well but generalises well to new data.
- **Optimal Model:** The goal is to have both low bias and low variance, which would result in a model that generalises well without overfitting.

---

### Visualisation of Bias and Variance
- **Bias:** A high-bias model would show a **consistent error** across different datasets. For example, a straight line fitting a non-linear data set would have high bias.
- **Variance:** A high-variance model would show **significant fluctuations** when trained on different data sets. For example, a deep decision tree might give wildly different results depending on the training data.

---

### Sources of Bias and Variance
- **Bias Sources:**
  - Using the wrong algorithm or a too-simple model.
  - Incorrect assumptions about the data.
  - Lack of features or poor feature engineering.

- **Variance Sources:**
  - Overly complex models with many parameters.
  - Too much noise in the training data.
  - Insufficient training data, causing the model to fit random patterns.

---

### Strategies to Reduce Bias and Variance

1. **Reducing Bias (Underfitting):**
   - Use more complex models (e.g., decision trees, neural networks).
   - Add more relevant features (feature engineering).
   - Remove excessive regularisation.

2. **Reducing Variance (Overfitting):**
   - Use simpler models (e.g., linear regression).
   - Add regularisation (L1, L2 regularisation).
   - Use cross-validation to detect overfitting.
   - Increase the size of the training dataset to ensure the model generalises well.

---

### Example: Bias-Variance Tradeoff in Decision Trees
- **High Bias (Underfitting):** A decision tree with a shallow depth (few splits) may not capture enough complexity in the data, leading to high bias.
- **High Variance (Overfitting):** A deep decision tree with many splits may perfectly fit the training data but fail to generalise to new data, leading to high variance.

---

### Example: Bias-Variance Tradeoff in Linear Regression
- **High Bias (Underfitting):** A simple linear regression model with one feature may not capture the true relationship in the data.
- **Low Bias, High Variance:** Adding more features might lower bias but can lead to high variance if there are irrelevant features or noise.

---

### Ideal Model
- **Balanced Bias and Variance:** The ideal model should have a small bias (captures key patterns) and low variance (generalises well to new data).
- **Cross-Validation:** Techniques like k-fold cross-validation help in identifying the optimal model that balances both bias and variance.

---

### Model Performance and Error Decomposition
The total error of a model can be decomposed into three parts:
1. **Bias Error:** The error introduced by incorrect assumptions in the model.
2. **Variance Error:** The error introduced by the model‚Äôs sensitivity to training data.
3. **Irreducible Error:** The error inherent in the data (noise), which cannot be reduced by the model.

$$
\text{Total Error} = \text{Bias}^2 + \text{Variance} + \text{Irreducible Error}
$$

#### üìù **NOTE:** Balancing bias and variance is crucial for building models that generalise well to new data. Understanding the tradeoff helps in selecting the right model and tuning it effectively.
---

## Decision Trees

### What is a Decision Tree?
A **Decision Tree** is a supervised machine learning algorithm used for classification and regression tasks. It divides the data into subsets based on the input features, creating a tree-like structure where each node represents a decision (split) based on a feature, and each leaf node represents an output label or value.

---

### Structure of a Decision Tree
- **Root Node:** Represents the entire dataset, from which the data is split.
- **Internal Nodes:** Represent decision points that split the data based on feature values.
- **Leaf Nodes:** Represent the output label or value (class or regression value) of the data in the subset.

---

### Working of a Decision Tree
1. **Selecting the Best Feature to Split:**
   At each node, the algorithm selects the feature that best splits the data. Common criteria include:
   - **Gini Impurity:** Measures the "impurity" of a dataset. Lower values indicate a better split.
   - **Entropy (Information Gain):** Measures the randomness or disorder. The algorithm chooses the feature that maximises information gain (i.e., reduces entropy the most).
   - **Mean Squared Error (for regression):** Measures the variance in the dataset. It‚Äôs used to minimise the variance at each node.

2. **Splitting the Data:**
   - The dataset is recursively split into subsets based on the chosen feature until a stopping criterion is met, such as:
     - A certain depth of the tree is reached.
     - All data points in a node belong to the same class.
     - A minimum number of data points is reached in a node.

---

### Gini Impurity and Entropy
1. **Gini Impurity (for Classification):**
   The Gini Impurity measures how often a randomly chosen element would be misclassified. The Gini score ranges from 0 to 1, where 0 means perfect purity (all data points in the node belong to one class).

   Formula for Gini Impurity:

   $$
   Gini = 1 - \sum_{i=1}^{k} p_i^2
   $$

   Where:
   - $$ p_i $$ is the proportion of data points in class $$ i $$.

2. **Entropy and Information Gain (for Classification):**
   Entropy measures the amount of uncertainty in the dataset. The goal is to reduce the entropy at each split.

   Formula for Entropy:

   $$
   Entropy = - \sum_{i=1}^{k} p_i \log_2(p_i)
   $$

   **Information Gain** is the difference between the entropy before and after a split:

   $$
   Information\ Gain = Entropy_{parent} - \sum_{i=1}^{n} \frac{|S_i|}{|S|} \cdot Entropy(S_i)
   $$

---

### Decision Trees for Regression
For regression problems, Decision Trees predict continuous values rather than class labels. Instead of using Gini or entropy, a regression tree uses the **Mean Squared Error (MSE)** to split the data.

Formula for MSE:

$$
MSE = \frac{1}{m} \sum_{i=1}^{m} (y_i - \hat{y})^2
$$

Where:
- $$ y_i $$ is the actual value.
- $$ \hat{y} $$ is the predicted value (mean of the target values in the node).

---

### Advantages of Decision Trees
1. **Easy to Understand and Interpret:**
   - Visual and interpretable structure.
   - Can be easily explained to non-experts.
   
2. **Handles Both Numerical and Categorical Data:**
   - Decision Trees can handle different types of features without requiring transformations.

3. **Non-linear Relationships:**
   - Can model complex, non-linear relationships between features and target variables.

4. **No Feature Scaling Required:**
   - Unlike algorithms such as SVMs or k-NN, Decision Trees do not require normalization or standardization of features.

---

### Disadvantages of Decision Trees
1. **Overfitting:**
   - Decision Trees can easily overfit the data if they are too deep or complex. Regularisation techniques like pruning and setting a maximum depth can help mitigate this.
   
2. **Instability:**
   - Small changes in the data can lead to a very different tree structure, making the model unstable.
   
3. **Bias Towards Features with More Levels:**
   - Decision Trees can favour features with many unique values, which may not always be the most relevant.

4. **Poor Performance on Extrapolation:**
   - Decision Trees are not good at extrapolating, meaning they may perform poorly on unseen data that falls outside the range of the training data.

---

### Pruning
Pruning is the process of removing parts of the tree that do not provide significant improvement in prediction accuracy. There are two types of pruning:
1. **Pre-Pruning (Early Stopping):**
   - Stops the tree from growing once a certain condition is met, such as a maximum depth or a minimum number of samples per node.
   
2. **Post-Pruning:**
   - Grows the tree fully and then removes branches that do not contribute much to the model‚Äôs accuracy.

---

### Hyperparameters in Decision Trees
- **Max Depth:** Limits the depth of the tree to prevent overfitting.
- **Min Samples Split:** The minimum number of samples required to split a node.
- **Min Samples Leaf:** The minimum number of samples required in a leaf node.
- **Max Features:** The maximum number of features to consider when splitting a node.

---

### Example of a Decision Tree for Classification
```python
from sklearn.tree import DecisionTreeClassifier

# Create the decision tree classifier
model = DecisionTreeClassifier(criterion='gini', max_depth=5)

# Fit the model to training data
model.fit(X_train, y_train)

# Make predictions
predictions = model.predict(X_test)
```

---

### Applications of Decision Trees
1. **Classification:**
   - Email Spam Detection (spam or not spam).
   - Disease Diagnosis (cancer or no cancer).
   
2. **Regression:**
   - Predicting house prices.
   - Forecasting sales based on features.

3. **Feature Selection:**
   - Decision Trees can help identify the most important features by examining the splits in the tree.

#### üìù **NOTE:** Decision Trees are powerful tools for both classification and regression, but they can suffer from overfitting. Regularisation techniques and proper hyperparameter tuning are crucial for building effective models.
---

## Naive Bayes

### What is Naive Bayes?
Naive Bayes is a supervised learning algorithm based on **Bayes' Theorem**. It is used for classification tasks and assumes that the features are independent given the class label. Despite its "naive" assumption of feature independence, Naive Bayes often performs surprisingly well, especially for text classification and spam detection.

---

### Bayes' Theorem
Bayes' Theorem provides a way to update the probability of a hypothesis based on new evidence. It is expressed as:

$$
P(C|X) = \frac{P(X|C) \cdot P(C)}{P(X)}
$$

Where:
- $$ P(C|X) $$: Posterior probability of class $$ C $$ given features $$ X $$.
- $$ P(X|C) $$: Likelihood, or the probability of features $$ X $$ given class $$ C $$.
- $$ P(C) $$: Prior probability of class $$ C $$.
- $$ P(X) $$: Evidence, or the probability of the features $$ X $$.

---

### Naive Assumption
The "naive" assumption in Naive Bayes is that the features are **conditionally independent** given the class. This simplifies the computation of the likelihood:

$$
P(X|C) = P(x_1, x_2, \dots, x_n | C) = P(x_1 | C) \cdot P(x_2 | C) \cdot \dots \cdot P(x_n | C)
$$

Where:
- $$ x_1, x_2, \dots, x_n $$ are the individual features in $$ X $$.

This assumption dramatically simplifies calculations, making Naive Bayes efficient even for large datasets.

---

### Types of Naive Bayes Models
1. **Gaussian Naive Bayes:**
   - Assumes that the features follow a **Gaussian (Normal) distribution**. It is used when the features are continuous.
   
   The probability density function for a Gaussian distribution is:

   $$
   P(x_i | C) = \frac{1}{\sqrt{2 \pi \sigma^2}} e^{-\frac{(x_i - \mu)^2}{2 \sigma^2}}
   $$

   Where $$ \mu $$ is the mean and $$ \sigma^2 $$ is the variance of the feature $$ x_i $$.

2. **Multinomial Naive Bayes:**
   - Used for **discrete count data**, such as word counts in text classification. This model assumes that the features are multinomially distributed.
   
   The probability mass function for the Multinomial distribution is:

   $$
   P(x i | C) = \frac{(n i)!}{((n i 1)! (n i 2)! ...)} \cdot p1^{n i 1} p2^{n i 2} ...
   $$

   Where $$ n_i $$ is the count of feature $$ x_i $$, and $$ p_i $$ is the probability of the feature value occurring.

3. **Bernoulli Naive Bayes:**
   - Similar to the Multinomial Naive Bayes but assumes that the features are **binary** (i.e., they take values 0 or 1). It is often used in text classification for tasks like spam detection.

---

### Model Training in Naive Bayes
1. **Prior Probability $$ P(C) $$:**
   - The probability of each class $$ C $$, computed from the frequency of classes in the training dataset.

   $$
   P(C) = \frac{n_C}{n_{\text{total}}}
   $$

   Where $$ n_C $$ is the number of samples in class $$ C $$, and $$ n_{\text{total}} $$ is the total number of samples.

2. **Likelihood $$ P(X|C) $$:**
   - The conditional probability of each feature given the class. This is estimated based on the training data.

   For continuous features (Gaussian Naive Bayes), this is the probability density function of the Gaussian distribution.  
   For discrete features (Multinomial and Bernoulli Naive Bayes), it is calculated as the frequency of feature occurrences within each class.

3. **Posterior Probability $$ P(C|X) $$:**
   - The model predicts the class $$ C $$ that maximizes the posterior probability:

   $$
   P(C|X) \propto P(C) \cdot P(X|C)
   $$

---

### Prediction
Once the model has been trained, for a new sample $$ X = (x_1, x_2, \dots, x_n) $$, the class $$ C $$ is predicted by selecting the class that maximizes the posterior probability:

$$
C_{\text{predicted}} = \arg\max_C P(C) \cdot P(x_1|C) \cdot P(x_2|C) \dots
$$

In practice, to avoid computationally expensive operations, we often use the logarithm of the probabilities:

$$
\log(P(C|X)) = \log(P(C)) + \sum_{i=1}^{n} \log(P(x_i | C))
$$

---

### Advantages of Naive Bayes
1. **Simple and Fast:** Naive Bayes is easy to implement and computationally efficient.
2. **Handles Large Datasets Well:** It is particularly useful when dealing with large datasets.
3. **Works Well with Text Data:** Naive Bayes is commonly used in text classification tasks, such as spam detection and sentiment analysis.
4. **Handles Missing Data:** It performs well even if some of the features are missing.

---

### Disadvantages of Naive Bayes
1. **Independence Assumption:** The main limitation is the assumption that features are conditionally independent, which is often not true in real-world data.
2. **Poor Performance with Highly Correlated Features:** If the features are highly correlated, Naive Bayes may perform poorly.
3. **Requires Sufficient Data for Each Feature:** It requires a sufficient amount of training data to estimate the probabilities for each feature properly.

---

### Applications of Naive Bayes
- **Text Classification:** Categorising emails as spam or not spam.
- **Sentiment Analysis:** Determining the sentiment of a text (positive/negative).
- **Medical Diagnosis:** Predicting whether a patient has a disease based on symptoms.
- **Recommendation Systems:** Classifying products or items based on user preferences.

---

### Example: Naive Bayes in Python
```python
from sklearn.naive_bayes import GaussianNB, MultinomialNB

# Create and train the Gaussian Naive Bayes model
model_gaussian = GaussianNB()
model_gaussian.fit(X_train, y_train)

# Make predictions
predictions_gaussian = model_gaussian.predict(X_test)

# Create and train the Multinomial Naive Bayes model
model_multinomial = MultinomialNB()
model_multinomial.fit(X_train, y_train)

# Make predictions
predictions_multinomial = model_multinomial.predict(X_test)
```

#### üìù **NOTE:** Despite its "naive" assumption of feature independence, Naive Bayes is a powerful and efficient algorithm, especially for text classification and other real-world applications.
---

## Support Vector Machines (SVM)

### What is Support Vector Machine (SVM)?
A **Support Vector Machine (SVM)** is a supervised machine learning algorithm used for classification and regression tasks. It is primarily used for binary classification, where it finds the optimal hyperplane that separates data into different classes. SVMs are particularly effective in high-dimensional spaces and for classification problems with clear margins of separation.

---

### Key Concepts of SVM
1. **Hyperplane:**
   - A hyperplane is a decision boundary that separates different classes in the feature space. In 2D, this is simply a line, but in higher dimensions, it becomes a plane or hyperplane.

2. **Support Vectors:**
   - The data points that are closest to the hyperplane and influence its position are called **support vectors**. These points are critical for determining the optimal hyperplane.

3. **Margin:**
   - The margin is the distance between the hyperplane and the support vectors. The goal of SVM is to maximize this margin, as a larger margin leads to better generalisation and less risk of overfitting.

4. **Optimal Hyperplane:**
   - The optimal hyperplane is the one that maximizes the margin while separating the classes. This is the hyperplane with the largest possible distance from the nearest support vector.

---

### SVM Mathematical Formulation
For a binary classification problem, SVM aims to find a hyperplane that separates two classes $$ C_1 $$ and $$ C_2 $$ while maximizing the margin. Given a dataset $$ (x_1, y_1), (x_2, y_2), \dots, (x_n, y_n) $$, where:
- $$ x_i $$ is the feature vector of the $$ i $$-th sample.
- $$ y_i $$ is the class label ($$ y_i \in \{-1, +1\} $$).

The equation of the hyperplane is:

$$
w \cdot x + b = 0
$$

Where:
- $$ w $$ is the weight vector (normal to the hyperplane).
- $$ b $$ is the bias (distance from the origin).

The margin is:

$$
\text{Margin} = \frac{2}{\|w\|}
$$

To maximize the margin, SVM minimizes the following objective function:

$$
\min_{w, b} \frac{1}{2} \|w\|^2
$$

Subject to the constraints:

$$
y_i (w \cdot x_i + b) \geq 1, \quad i = 1, 2, \dots, n
$$

This ensures that the points are correctly classified and lie outside the margin.

---

### Types of SVM
1. **Linear SVM:**
   - Used when the data is linearly separable. The algorithm tries to find a straight line (in 2D) or hyperplane (in higher dimensions) that separates the two classes.

2. **Non-linear SVM:**
   - When the data is not linearly separable, SVM uses the **kernel trick** to map the data into a higher-dimensional space where a linear hyperplane can be found. Common kernels include:
     - **Polynomial Kernel**: $$ K(x, x') = (x \cdot x' + 1)^d $$
     - **Radial Basis Function (RBF) Kernel (Gaussian Kernel)**: $$ K(x, x') = e^{-\frac{\|x - x'\|^2}{2\sigma^2}} $$
     - **Sigmoid Kernel**: $$ K(x, x') = \tanh(\alpha x \cdot x' + c) $$

   By using the kernel trick, SVM implicitly maps the data into a higher-dimensional feature space without computing the mapping explicitly, making it computationally efficient.

---

### SVM for Regression (SVR)
SVM can also be adapted for regression problems, called **Support Vector Regression (SVR)**. The goal of SVR is to find a hyperplane that best fits the data while allowing some margin of error. The margin is controlled by a parameter $$ \epsilon $$, which defines the acceptable error margin.

---

### Advantages of SVM
1. **Effective in High Dimensional Spaces:**
   - SVMs perform well when the number of features is large relative to the number of data points.

2. **Robust to Overfitting:**
   - SVMs are less prone to overfitting, especially when the margin is maximized, as they focus on the most important support vectors.

3. **Versatility:**
   - With the use of different kernel functions, SVM can efficiently perform both linear and non-linear classification tasks.

4. **Clear Geometric Interpretation:**
   - The concept of support vectors and the margin provides a clear geometric understanding of the model.

---

### Disadvantages of SVM
1. **Computationally Expensive:**
   - SVMs can be slow to train, especially for large datasets, as they involve solving a quadratic optimization problem.
   
2. **Sensitive to Choice of Kernel and Parameters:**
   - The performance of an SVM heavily depends on the choice of kernel function and the regularization parameter $$ C $$, as well as the kernel parameters such as $$ \sigma $$ in the RBF kernel.

3. **Poor Performance with Noisy Data:**
   - SVMs can perform poorly if the data contains a lot of noise or overlaps in class distributions.

---

### Hyperparameters in SVM
1. **C (Regularization Parameter):**
   - Controls the tradeoff between maximizing the margin and minimizing classification error. A large $$ C $$ results in a smaller margin but fewer misclassifications, while a smaller $$ C $$ increases the margin but allows some misclassification.

2. **Kernel Type:**
   - Specifies the type of kernel to be used (e.g., linear, polynomial, RBF, sigmoid).

3. **Gamma (for RBF kernel):**
   - Determines the influence of a single training example. A high $$ \gamma $$ value means a small influence, while a low $$ \gamma $$ means the influence is more spread out.

4. **Degree (for Polynomial kernel):**
   - Defines the degree of the polynomial kernel, influencing the model's complexity.

---

### Example: SVM in Python
```python
from sklearn.svm import SVC
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split

# Generate synthetic data
X, y = make_classification(n_samples=100, n_features=2, n_classes=2)

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)

# Create and train the SVM model
model = SVC(kernel='linear', C=1)
model.fit(X_train, y_train)

# Make predictions
predictions = model.predict(X_test)
```

---

### Applications of SVM
- **Image Classification:** Recognizing handwritten digits or objects in images.
- **Text Classification:** Spam filtering, sentiment analysis, and topic categorization.
- **Bioinformatics:** Gene classification and disease prediction.
- **Face Recognition:** Classifying faces in images.
- **Financial Prediction:** Predicting stock prices or credit card fraud.

#### üìù **NOTE:** SVMs are powerful, particularly in high-dimensional spaces. With proper tuning of the kernel and parameters, SVM can be an effective classifier, even for complex datasets.
---

## Kernel Methods

### What are Kernel Methods?
**Kernel methods** are a class of algorithms used in machine learning that enable non-linear transformations of data into higher-dimensional feature spaces without explicitly computing the transformation. The most famous use of kernel methods is in **Support Vector Machines (SVM)**, but they are also applied in algorithms like **Principal Component Analysis (PCA)** and **Gaussian Processes**.

The central idea behind kernel methods is to compute the **kernel function**, which computes the inner product between data points in a higher-dimensional feature space, without explicitly mapping the data points to that space.

---

### The Kernel Trick
The **kernel trick** allows for the efficient computation of the inner product of data points in a higher-dimensional feature space without the need to calculate the coordinates of the data points in that space explicitly.

For a given set of data points $$ x $$ and $$ x' $$, the kernel function computes the inner product of $$ \phi(x) $$ and $$ \phi(x') $$, where $$ \phi $$ represents the mapping to the higher-dimensional space:

$$
K(x, x') = \langle \phi(x), \phi(x') \rangle
$$

By using kernel functions, the algorithm can work in higher-dimensional spaces (potentially infinite-dimensional) without the need to compute expensive transformations or store the transformed data.

---

### Common Kernel Functions
1. **Linear Kernel:**
   - The simplest kernel, equivalent to the dot product of the data points in the original feature space.
   - Formula:  
     $$
     K(x, x') = x \cdot x'
     $$

   - The linear kernel is useful when the data is already linearly separable.

2. **Polynomial Kernel:**
   - Maps the data into a higher-dimensional space using polynomial functions. It is useful for capturing interactions between features.
   - Formula:  
     $$
     K(x, x') = (x \cdot x' + c)^d
     $$

   Where:
   - $$ d $$: Degree of the polynomial.
   - $$ c $$: A constant that controls the offset.

3. **Radial Basis Function (RBF) or Gaussian Kernel:**
   - The most commonly used kernel, especially for non-linear data. It maps the data to an infinite-dimensional space.
   - Formula:  
     $$
     K(x, x') = e^{-\frac{\|x - x'\|^2}{2\sigma^2}}
     $$

   Where:
   - $$ \|x - x'\|^2 $$ is the squared Euclidean distance between the data points.
   - $$ \sigma $$ is a parameter that controls the width of the Gaussian.

4. **Sigmoid Kernel:**
   - Similar to the activation function used in neural networks, this kernel is less commonly used but can be applied in some contexts.
   - Formula:  
     $$
     K(x, x') = \tanh(\alpha x \cdot x' + c)
     $$

   Where:
   - $$ \alpha $$ is a scaling parameter, and $$ c $$ is an offset.

5. **Laplacian Kernel:**
   - Similar to the RBF kernel but uses the Manhattan (L1) distance rather than the squared Euclidean distance.
   - Formula:  
     $$
     K(x, x') = e^{-\frac{\|x - x'\|_1}{\sigma}}
     $$

   Where:
   - $$ \|x - x'\|_1 $$ is the L1 distance between $$ x $$ and $$ x' $$.

---

### Advantages of Kernel Methods
1. **Non-linear Mapping:**  
   Kernel methods allow learning from non-linearly separable data by transforming it into higher dimensions where a linear separation is possible.

2. **Efficient Computation:**  
   The kernel trick enables the computation of inner products in a higher-dimensional space without explicitly mapping the data, saving computational resources.

3. **Flexibility:**  
   Different kernel functions can be chosen based on the data distribution and problem type, giving kernel methods flexibility to handle a wide variety of problems.

4. **Versatility:**  
   Kernel methods can be applied to many machine learning algorithms, including classification, regression, and dimensionality reduction.

---

### Disadvantages of Kernel Methods
1. **Computational Complexity:**  
   While the kernel trick reduces dimensionality issues, it still requires computing the kernel matrix, which can be computationally expensive for large datasets (since it requires calculating pairwise distances between all data points).

2. **Choice of Kernel:**  
   The choice of kernel and its parameters (e.g., $$ \sigma $$ for the RBF kernel) significantly impacts the performance of the model. Finding the right kernel can be challenging.

3. **Memory Usage:**  
   The kernel matrix can grow large, especially when working with a large number of samples, requiring significant memory storage.

4. **Overfitting:**  
   Using powerful kernels, especially with small datasets, can lead to overfitting. Regularization is necessary to prevent this.

---

### Applications of Kernel Methods
1. **Support Vector Machines (SVM):**
   - SVMs with kernel methods are widely used for classification and regression tasks, especially when data is not linearly separable.

2. **Principal Component Analysis (PCA):**
   - Kernel PCA is an extension of PCA that uses kernel methods to find principal components in non-linear spaces.

3. **Gaussian Processes:**
   - Kernel methods are also used in Gaussian Processes, which provide a non-parametric approach to regression and classification.

4. **Clustering (e.g., Kernel K-means):**
   - Kernel K-means uses kernel functions to perform clustering in non-linear spaces, providing a more flexible alternative to traditional K-means.

5. **Time Series Analysis:**
   - Kernel methods are used in time series forecasting and anomaly detection to handle complex, non-linear relationships.

---

### Example of SVM with RBF Kernel in Python
```python
from sklearn.svm import SVC
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split

# Generate synthetic data
X, y = make_classification(n_samples=100, n_features=2, n_classes=2)

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)

# Create and train the SVM model with RBF kernel
model = SVC(kernel='rbf', C=1, gamma=0.5)
model.fit(X_train, y_train)

# Make predictions
predictions = model.predict(X_test)
```

---

### Hyperparameters in Kernel Methods
1. **Kernel Choice:**  
   The selection of kernel (e.g., RBF, polynomial, sigmoid) depends on the problem and the characteristics of the data.

2. **Regularization Parameter (C):**  
   Controls the trade-off between maximizing the margin and minimizing classification error. A small $$ C $$ leads to a larger margin but allows some misclassification.

3. **Gamma (for RBF kernel):**  
   Controls the influence of a single training example. A low $$ \gamma $$ leads to a larger decision boundary, while a high $$ \gamma $$ makes the decision boundary more sensitive to individual data points.

4. **Degree (for Polynomial kernel):**  
   Controls the degree of the polynomial in the polynomial kernel. A higher degree increases the model's complexity.

#### üìù **NOTE:** Kernel methods are powerful tools for solving complex, non-linear problems, but selecting the right kernel and tuning hyperparameters are crucial steps for achieving optimal performance.
---