---
title: "Unit 3: Statistical Analysis"
description: Discrete Distribution- Binomial, Poisson distribution with mean variance, Moment generating function.Continuous Distribution- Normal and Exponential Distribution with mean variance, Moment generating function.
date: 2025-01-19
tags: ["Statistical Analysis", "4th Semester", "2nd Year", "medicaps university"]
published: true
metadata:
  university: "Medicaps University"
  degree: "B Tech"
  semester: "4th Semester"
  subject: "Statistical Analysis"
---

## Discrete Distribution

---

### 1. Introduction to Discrete Distribution

A **discrete probability distribution** is a statistical distribution where the random variable can take on a countable number of distinct values. These values may be finite or countably infinite. Discrete distributions are used to model scenarios where outcomes can be counted, such as the number of heads in a series of coin flips or the number of customers arriving at a store.

The probability mass function (PMF) is used to define the probability of each outcome in a discrete distribution. In discrete distributions, the sum of all probabilities must be equal to 1.

---

### 2. Probability Mass Function (PMF)

The **Probability Mass Function (PMF)** is a function that gives the probability that a discrete random variable is exactly equal to some value. For a discrete random variable $$ X $$, the PMF is denoted as $$ P(X = x) $$, where $$ x $$ is a possible value of the random variable $$ X $$.

The PMF must satisfy two conditions:
- $$ 0 \leq P(X = x) \leq 1 $$ for all $$ x $$.
- The sum of all probabilities must equal 1:

$$ \sum_{x} P(X = x) = 1 $$

#### Example

Consider a fair six-sided die. The random variable $$ X $$ represents the outcome of a single roll, where $$ X \in \{1, 2, 3, 4, 5, 6\} $$. The PMF of $$ X $$ is:

$$ P(X = x) = \frac{1}{6} \text{ for each } x \in \{1, 2, 3, 4, 5, 6\} $$

The sum of all probabilities is:

$$ \sum_{x=1}^{6} P(X = x) = 6 \times \frac{1}{6} = 1 $$

---

### 3. Types of Discrete Distributions

There are several types of discrete distributions used in probability and statistics. Some of the most commonly used discrete distributions include:

#### 3.1 Bernoulli Distribution

The **Bernoulli distribution** models a random experiment with exactly two possible outcomes: "success" and "failure". A Bernoulli trial is an experiment with only two outcomes, such as flipping a coin (Heads or Tails).

The probability mass function of a Bernoulli random variable $$ X $$ is given by:

$$ P(X = 1) = p \quad \text{and} \quad P(X = 0) = 1 - p $$

Where $$ p $$ is the probability of success (usually $$ 0 \leq p \leq 1 $$).

##### Example

For a biased coin where the probability of heads (success) is 0.7, the PMF is:

$$ P(X = 1) = 0.7 \quad \text{and} \quad P(X = 0) = 0.3 $$

#### 3.2 Binomial Distribution

The **binomial distribution** models the number of successes in a fixed number of independent Bernoulli trials, each with the same probability of success. It is commonly used in scenarios such as the number of heads in a series of coin flips.

The probability mass function of a binomial random variable $$ X $$ is:

$$ P(X = k) = \binom{n}{k} p^k (1-p)^{n-k} $$

Where:
- $$ n $$ is the number of trials,
- $$ k $$ is the number of successes,
- $$ p $$ is the probability of success.

##### Example

If a coin is flipped 5 times, and the probability of heads (success) is 0.5, the probability of getting exactly 3 heads is:

$$ P(X = 3) = \binom{5}{3} (0.5)^3 (0.5)^{5-3} = \frac{5!}{3!(5-3)!} \cdot (0.5)^5 = 10 \cdot \frac{1}{32} = \frac{5}{16} $$

#### 3.3 Poisson Distribution

The **Poisson distribution** models the number of events that occur within a fixed interval of time or space, given that the events happen independently and at a constant average rate.

The probability mass function of a Poisson random variable $$ X $$ is:

$$ P(X = k) = \frac{\lambda^k e^{-\lambda}}{k!} $$

Where:
- $$ \lambda $$ is the average rate of occurrence of events,
- $$ k $$ is the number of events,
- $$ e $$ is Euler's number (approximately 2.71828).

##### Example

If a call center receives an average of 3 calls per hour, the probability of receiving exactly 2 calls in an hour is:

$$ P(X = 2) = \frac{3^2 e^{-3}}{2!} = \frac{9 \cdot e^{-3}}{2} \approx 0.2241 $$

---

### 4. Expectation and Variance of Discrete Distributions

#### 4.1 Expectation (Mean)

The **expectation** or **mean** of a discrete random variable $$ X $$, denoted as $$ E(X) $$, is the weighted average of all possible values of $$ X $$, where the weights are the probabilities of those values. The expectation is calculated as:

$$ E(X) = \sum_{x} x \cdot P(X = x) $$

#### 4.2 Variance

The **variance** of a discrete random variable $$ X $$, denoted as $$ \text{Var}(X) $$, measures the spread of the possible values around the mean. It is calculated as:

$$ \text{Var}(X) = E(X^2) - (E(X))^2 $$

Where $$ E(X^2) $$ is the expected value of the square of $$ X $$.

For a binomial distribution, the variance is given by:

$$ \text{Var}(X) = n \cdot p \cdot (1 - p) $$

---

### 5. Cumulative Distribution Function (CDF)

The **Cumulative Distribution Function (CDF)** of a discrete random variable $$ X $$ is the probability that $$ X $$ will take a value less than or equal to a particular value $$ x $$. It is given by:

$$ F_X(x) = P(X \leq x) = \sum_{i \leq x} P(X = i) $$

Where $$ F_X(x) $$ is the CDF of $$ X $$ at point $$ x $$.

---

### 6. Summary

- A **discrete distribution** describes the probability distribution of a discrete random variable, where the variable can take on a countable number of distinct values.
- The **Probability Mass Function (PMF)** gives the probability of each possible outcome.
- Common discrete distributions include the **Bernoulli**, **Binomial**, and **Poisson** distributions.
- The **expectation** is the weighted average of all possible values of the random variable, and the **variance** measures the spread of the distribution.
- The **Cumulative Distribution Function (CDF)** gives the probability that the random variable is less than or equal to a specific value.

---

**üí° TIP:** Discrete distributions are used in a wide range of real-world scenarios, from modeling the number of heads in coin flips to the number of occurrences of events in a given time frame (e.g., phone calls, accidents, or server requests).

**üìù NOTE:** The **Poisson distribution** is often used when events happen independently and at a constant rate over time, while the **binomial distribution** is used for a fixed number of independent trials.

**‚ö†Ô∏è CAUTION:** When working with discrete distributions, ensure that the sum of all probabilities in the PMF equals 1. If not, there may be an error in your calculations.

---

## Moment Generating Function (MGF)

---

### 1. Introduction to Moment Generating Function (MGF)

The **Moment Generating Function (MGF)** is a function that provides a way to compute all the moments (e.g., mean, variance, skewness, kurtosis) of a random variable. It is particularly useful because it encapsulates the distribution of the random variable and can be used to find various properties of the distribution, such as its mean and variance.

The MGF of a random variable $$ X $$, denoted by $$ M_X(t) $$, is defined as the expected value of $$ e^{tX} $$:

$$ M_X(t) = E[e^{tX}] $$

Where:
- $$ t $$ is a real number,
- $$ E[e^{tX}] $$ is the expected value of $$ e^{tX} $$.

The MGF is useful for deriving moments and for solving problems related to distributions and random variables.

---

### 2. Properties of Moment Generating Function

The Moment Generating Function has several important properties that make it a powerful tool in probability theory and statistics:

#### 2.1 Existence of MGF

For a random variable $$ X $$, the MGF $$ M_X(t) $$ exists if the expectation $$ E[e^{tX}] $$ exists for some range of values of $$ t $$. If the MGF exists, it can be used to compute the moments of the distribution.

#### 2.2 Moments of the Distribution

The MGF is directly related to the moments of the random variable $$ X $$. The $$ n $$-th moment of $$ X $$, denoted by $$ E[X^n] $$, can be found by differentiating the MGF:

$$ E[X^n] = \left. \frac{d^n M_X(t)}{dt^n} \right|_{t=0} $$

In other words, the $$ n $$-th moment is the $$ n $$-th derivative of the MGF evaluated at $$ t = 0 $$.

- The **first moment** (mean) is:

  $$ E[X] = \left. \frac{d M_X(t)}{dt} \right|_{t=0} $$

- The **second moment** is:

  $$ E[X^2] = \left. \frac{d^2 M_X(t)}{dt^2} \right|_{t=0} $$

- The **variance** is given by:

  $$ \text{Var}(X) = E[X^2] - (E[X])^2 $$

#### 2.3 Uniqueness of MGF

The MGF, if it exists, uniquely determines the distribution of the random variable $$ X $$. In other words, two different random variables with the same MGF must have the same distribution. This property is especially useful in proving distributional results.

#### 2.4 Cumulant Generating Function

The **cumulant generating function** is the natural logarithm of the MGF. It is useful for calculating the cumulants, which are another set of distributional parameters related to the moments but provide additional insight into the shape of the distribution.

$$ K_X(t) = \log(M_X(t)) $$

Where $$ K_X(t) $$ is the cumulant generating function.

---

### 3. Example of Moment Generating Function

Let us consider a **Bernoulli distribution** with parameter $$ p $$, where the random variable $$ X $$ takes values 0 or 1, with probabilities $$ P(X = 1) = p $$ and $$ P(X = 0) = 1 - p $$.

The MGF for $$ X $$ is:

$$ M_X(t) = E[e^{tX}] = p e^{t} + (1 - p) e^{0} = p e^{t} + (1 - p) $$

We can use this MGF to compute the moments:

- The **first moment (mean)** is:

  $$ E[X] = \frac{d}{dt} M_X(t) \Bigg|_{t=0} = \frac{d}{dt} (p e^{t} + (1 - p)) \Bigg|_{t=0} = p $$

- The **second moment** is:

  $$ E[X^2] = \frac{d^2}{dt^2} M_X(t) \Bigg|_{t=0} = p $$

- The **variance** is:

  $$ \text{Var}(X) = E[X^2] - (E[X])^2 = p - p^2 = p(1 - p) $$

Thus, the MGF helps in calculating the mean and variance of the Bernoulli distribution easily.

---

### 4. Common Distributions and Their Moment Generating Functions

#### 4.1 MGF of the Normal Distribution

For a **normal distribution** with mean $$ \mu $$ and variance $$ \sigma^2 $$, the MGF is given by:

$$ M_X(t) = \exp \left( \mu t + \frac{\sigma^2 t^2}{2} \right) $$

This MGF can be used to derive the moments of the normal distribution.

#### 4.2 MGF of the Exponential Distribution

For an **exponential distribution** with rate parameter $$ \lambda $$, the MGF is:

$$ M_X(t) = \frac{\lambda}{\lambda - t} \quad \text{for } t < \lambda $$

This MGF is useful for calculating the moments of the exponential distribution.

---

### 5. Applications of Moment Generating Function

- **Deriving moments**: The MGF provides a straightforward way to calculate moments (mean, variance, higher moments) of random variables.
- **Characterizing distributions**: Since the MGF uniquely determines the distribution (if it exists), it can be used to identify the distribution of a random variable.
- **Solving for the distribution of sums**: The MGF is useful for finding the distribution of the sum of independent random variables. If $$ X_1, X_2, \dots, X_n $$ are independent random variables with MGFs $$ M_{X_1}(t), M_{X_2}(t), \dots, M_{X_n}(t) $$, then the MGF of the sum $$ S = X_1 + X_2 + \dots + X_n $$ is:

  $$ M_S(t) = M_{X_1}(t) \cdot M_{X_2}(t) \cdot \dots \cdot M_{X_n}(t) $$

- **Central Limit Theorem (CLT)**: The CLT is based on the MGF and states that the distribution of the sum (or average) of a large number of independent and identically distributed (i.i.d.) random variables approaches a normal distribution, regardless of the original distribution.

---

### 6. Summary

- The **Moment Generating Function (MGF)** is defined as $$ M_X(t) = E[e^{tX}] $$ and provides a way to compute all the moments of a random variable.
- The **first derivative** of the MGF gives the **mean**, and higher derivatives provide higher moments (e.g., variance, skewness).
- The MGF **uniquely determines** the distribution of a random variable (if it exists).
- The MGF is used in various applications, such as solving for the distribution of sums of random variables and proving the Central Limit Theorem.

---

**üí° TIP:** The Moment Generating Function is a powerful tool for computing moments and analyzing distributions, but it is important to check whether the MGF exists for the distribution in question.

**üìù NOTE:** If you are dealing with a random variable whose distribution is difficult to work with directly, using the MGF can simplify calculations, especially when dealing with sums of random variables.

**‚ö†Ô∏è CAUTION:** The Moment Generating Function may not always exist for every distribution. For some distributions, the MGF may not be finite for all values of $$ t $$.

---

## Continuous Distribution

---

### 1. Introduction to Continuous Distribution

A **continuous distribution** describes the probability of a random variable taking a value within a continuous range. Unlike discrete distributions, where the random variable takes specific values, a continuous random variable can take any value in a given interval. These distributions are described using probability density functions (PDF), which allow us to calculate the probability of a random variable falling within a particular range.

A continuous random variable $$ X $$ is defined by its probability density function (PDF), denoted by $$ f_X(x) $$. The probability that $$ X $$ takes a value within the interval $$ [a, b] $$ is given by the area under the curve of the PDF between $$ a $$ and $$ b $$:

$$ P(a \leq X \leq b) = \int_a^b f_X(x) \, dx $$

The total area under the probability density function must equal 1, i.e.,

$$ \int_{-\infty}^{\infty} f_X(x) \, dx = 1 $$

---

### 2. Properties of Continuous Distributions

#### 2.1 Probability Density Function (PDF)

For a continuous random variable $$ X $$, the PDF $$ f_X(x) $$ must satisfy the following properties:

- $$ f_X(x) \geq 0 $$ for all $$ x $$.
- The total area under the curve of the PDF is 1:

  $$ \int_{-\infty}^{\infty} f_X(x) \, dx = 1 $$

- The probability that $$ X $$ takes a specific value is 0:

  $$ P(X = x) = 0 $$

Since the probability of a random variable taking a specific value is zero, probabilities are calculated over intervals.

#### 2.2 Cumulative Distribution Function (CDF)

The **Cumulative Distribution Function (CDF)**, denoted by $$ F_X(x) $$, gives the probability that the random variable $$ X $$ takes a value less than or equal to $$ x $$:

$$ F_X(x) = P(X \leq x) = \int_{-\infty}^{x} f_X(t) \, dt $$

The CDF is a non-decreasing function and approaches 1 as $$ x \to \infty $$ and 0 as $$ x \to -\infty $$.

#### 2.3 Mean and Variance of a Continuous Distribution

The **mean** (or expected value) $$ E[X] $$ of a continuous random variable $$ X $$ is given by:

$$ E[X] = \int_{-\infty}^{\infty} x f_X(x) \, dx $$

The **variance** $$ \text{Var}(X) $$ is given by:

$$ \text{Var}(X) = E[X^2] - (E[X])^2 $$

Where $$ E[X^2] $$ is the second moment of $$ X $$:

$$ E[X^2] = \int_{-\infty}^{\infty} x^2 f_X(x) \, dx $$

---

### 3. Common Continuous Distributions

#### 3.1 Uniform Distribution

A **Uniform Distribution** is a continuous distribution where all values in a given interval are equally likely. If a random variable $$ X $$ is uniformly distributed between $$ a $$ and $$ b $$, its PDF is:

$$ f_X(x) = \frac{1}{b - a}, \quad \text{for} \, a \leq x \leq b $$

The CDF is:

$$ F_X(x) = \frac{x - a}{b - a}, \quad \text{for} \, a \leq x \leq b $$

The mean and variance for a uniform distribution are:

- **Mean**: $$ E[X] = \frac{a + b}{2} $$
- **Variance**: $$ \text{Var}(X) = \frac{(b - a)^2}{12} $$

#### 3.2 Normal Distribution

The **Normal Distribution** is one of the most important continuous distributions in statistics. It is characterized by its mean $$ \mu $$ and variance $$ \sigma^2 $$. The PDF of the normal distribution is:

$$ f_X(x) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp \left( -\frac{(x - \mu)^2}{2 \sigma^2} \right) $$

The CDF is the integral of the PDF and has no simple closed-form expression, but it can be computed using standard normal tables or software.

The mean and variance for a normal distribution are:

- **Mean**: $$ E[X] = \mu $$
- **Variance**: $$ \text{Var}(X) = \sigma^2 $$

#### 3.3 Exponential Distribution

The **Exponential Distribution** is often used to model the time between events in a Poisson process. Its PDF is:

$$ f_X(x) = \lambda e^{-\lambda x}, \quad x \geq 0 $$

Where $$ \lambda $$ is the rate parameter. The CDF is:

$$ F_X(x) = 1 - e^{-\lambda x}, \quad x \geq 0 $$

The mean and variance for an exponential distribution are:

- **Mean**: $$ E[X] = \frac{1}{\lambda} $$
- **Variance**: $$ \text{Var}(X) = \frac{1}{\lambda^2} $$

---

### 4. Applications of Continuous Distributions

- **Modeling Real-World Phenomena**: Continuous distributions are often used to model real-world phenomena where data can take any value within a certain range, such as the height of individuals, the amount of time it takes to complete a task, or the temperature in a city.
- **Statistical Inference**: Continuous distributions form the basis for many statistical methods, such as hypothesis testing, confidence intervals, and regression analysis.
- **Simulation**: Continuous distributions are used in simulations to generate random samples of data from specific distributions (e.g., using Monte Carlo simulations).

---

### 5. Summary

- A **continuous distribution** is a probability distribution where the random variable can take any value within a given interval, and the probabilities are defined over ranges of values.
- The **probability density function (PDF)** describes the likelihood of a random variable taking a value in a given range, and its total area under the curve is 1.
- The **Cumulative Distribution Function (CDF)** is used to calculate the probability that a random variable is less than or equal to a specific value.
- Common continuous distributions include the **Uniform Distribution**, **Normal Distribution**, and **Exponential Distribution**, each with its own characteristics, mean, and variance formulas.

---

**üí° TIP:** Understanding the properties and applications of continuous distributions is crucial for modeling real-world data and performing statistical analysis.

**üìù NOTE:** For complex problems involving continuous distributions, it is often necessary to use numerical methods or software tools to calculate probabilities and moments.

**‚ö†Ô∏è CAUTION:** While continuous distributions model many real-world phenomena, the key difference from discrete distributions is that continuous random variables take on infinitely many values, so we always work with intervals rather than specific values.

---

## Normal and Exponential Distribution: Mean, Variance, Moment Generating Function

---

### 1. Normal Distribution

#### 1.1 Introduction to Normal Distribution

The **Normal Distribution** (also known as Gaussian Distribution) is one of the most important and widely used continuous probability distributions in statistics. It is often used to model real-world data due to its well-known bell-shaped curve. The normal distribution is fully described by two parameters: the **mean (¬µ)** and the **variance (œÉ¬≤)**.

The **probability density function (PDF)** of a normal distribution is given by:

$$ f_X(x) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp \left( -\frac{(x - \mu)^2}{2 \sigma^2} \right) $$

Where:
- $$ \mu $$ is the mean (center of the distribution),
- $$ \sigma^2 $$ is the variance (spread of the distribution),
- $$ \pi $$ is the constant 3.1416, and
- $$ \exp $$ represents the exponential function.

#### 1.2 Mean and Variance of Normal Distribution

The **mean** ($$ E[X] $$) and **variance** ($$ \text{Var}(X) $$) of a normal distribution are given by:

- **Mean**:

  $$ E[X] = \mu $$

- **Variance**:

  $$ \text{Var}(X) = \sigma^2 $$

These two parameters determine the location (mean) and spread (variance) of the normal distribution.

#### 1.3 Moment Generating Function of Normal Distribution

The **Moment Generating Function (MGF)** of a random variable $$ X $$ is defined as $$ M_X(t) = E[e^{tX}] $$. For a normal distribution, the MGF is derived as follows:

$$ M_X(t) = E[e^{tX}] = \int_{-\infty}^{\infty} e^{tx} f_X(x) \, dx $$

Substituting the normal PDF, we get:

$$ M_X(t) = \exp \left( \mu t + \frac{\sigma^2 t^2}{2} \right) $$

This is the MGF of a normal distribution, which provides a convenient way to compute moments.

The **n-th moment** of the normal distribution can be obtained by differentiating the MGF:

$$ E[X^n] = \left. \frac{d^n M_X(t)}{dt^n} \right|_{t=0} $$

For example:
- The **mean** is $$ E[X] = \mu $$,
- The **variance** is $$ \text{Var}(X) = \sigma^2 $$.

#### 1.4 Properties of Normal Distribution

- The **normal distribution** is symmetric about its mean $$ \mu $$.
- The **68-95-99.7 Rule**: 
  - About 68% of the data lies within one standard deviation of the mean.
  - About 95% lies within two standard deviations.
  - About 99.7% lies within three standard deviations.
- The **total area under the curve** is 1, meaning the probability of the random variable taking any value within the entire distribution is 1.

---

### 2. Exponential Distribution

#### 2.1 Introduction to Exponential Distribution

The **Exponential Distribution** is commonly used to model the time between events in a Poisson process, where events occur continuously and independently at a constant average rate. It is a one-parameter distribution, and the random variable $$ X $$ is typically positive.

The **probability density function (PDF)** of an exponential distribution is:

$$ f_X(x) = \lambda e^{-\lambda x}, \quad x \geq 0 $$

Where $$ \lambda > 0 $$ is the rate parameter, representing the rate at which events occur. The exponential distribution is often used in scenarios such as the time between arrivals of customers in a queue or the lifespan of a light bulb.

#### 2.2 Mean and Variance of Exponential Distribution

The **mean** ($$ E[X] $$) and **variance** ($$ \text{Var}(X) $$) of an exponential distribution are:

- **Mean**:

  $$ E[X] = \frac{1}{\lambda} $$

- **Variance**:

  $$ \text{Var}(X) = \frac{1}{\lambda^2} $$

Thus, the mean is the inverse of the rate parameter, and the variance is the square of the inverse of the rate parameter.

#### 2.3 Moment Generating Function of Exponential Distribution

The **Moment Generating Function (MGF)** of a random variable $$ X $$ is $$ M_X(t) = E[e^{tX}] $$. For an exponential distribution, the MGF is derived as follows:

$$ M_X(t) = E[e^{tX}] = \int_0^\infty e^{tx} \lambda e^{-\lambda x} \, dx $$

Simplifying the integral:

$$ M_X(t) = \frac{\lambda}{\lambda - t}, \quad \text{for} \, t < \lambda $$

This is the MGF of an exponential distribution, and it can be used to find the moments of the distribution.

For example:
- The **mean** is $$ E[X] = \frac{1}{\lambda} $$,
- The **variance** is $$ \text{Var}(X) = \frac{1}{\lambda^2} $$.

#### 2.4 Properties of Exponential Distribution

- The **exponential distribution** is memoryless, meaning that the probability of an event occurring in the future is independent of the past. In mathematical terms:

  $$ P(X > x + y | X > y) = P(X > x) $$

- The **mean** and **variance** are directly related to the rate parameter $$ \lambda $$.
- The exponential distribution is **skewed to the right**, with a longer tail on the right side of the mean.

---

### 3. Summary

- The **Normal Distribution** is a symmetric, bell-shaped distribution characterized by its mean $$ \mu $$ and variance $$ \sigma^2 $$. The MGF of a normal distribution is $$ M_X(t) = \exp \left( \mu t + \frac{\sigma^2 t^2}{2} \right) $$.
- The **Exponential Distribution** models the time between events in a Poisson process. It is characterized by the rate parameter $$ \lambda $$, with the PDF $$ f_X(x) = \lambda e^{-\lambda x} $$. The MGF of an exponential distribution is $$ M_X(t) = \frac{\lambda}{\lambda - t} $$.

Both distributions have well-defined properties and are widely used in various fields, including engineering, economics, and natural sciences.

---

**üí° TIP:** The normal distribution is a fundamental concept in statistics and is often used as an approximation for other distributions due to the Central Limit Theorem.

**üìù NOTE:** The exponential distribution‚Äôs memoryless property makes it particularly useful in modeling processes where future events are independent of past events.

**‚ö†Ô∏è CAUTION:** The moment generating function for the exponential distribution only exists for $$ t < \lambda $$, so it‚Äôs important to check the range of $$ t $$ before using the MGF.

---

